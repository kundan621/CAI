{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb997ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Aug 22, 2025 11:22:08 PM org.apache.pdfbox.rendering.PDFRenderer suggestKCMS\n",
      "INFO: Your current java version is: 1.8.0_161\n",
      "Aug 22, 2025 11:22:08 PM org.apache.pdfbox.rendering.PDFRenderer suggestKCMS\n",
      "INFO: To get higher rendering speed on old java 1.8 or 9 versions,\n",
      "Aug 22, 2025 11:22:08 PM org.apache.pdfbox.rendering.PDFRenderer suggestKCMS\n",
      "INFO:   update to the latest 1.8 or 9 version (>= 1.8.0_191 or >= 9.0.4),\n",
      "Aug 22, 2025 11:22:08 PM org.apache.pdfbox.rendering.PDFRenderer suggestKCMS\n",
      "INFO:   or\n",
      "Aug 22, 2025 11:22:08 PM org.apache.pdfbox.rendering.PDFRenderer suggestKCMS\n",
      "INFO:   use the option -Dsun.java2d.cmm=sun.java2d.cmm.kcms.KcmsServiceProvider\n",
      "Aug 22, 2025 11:22:08 PM org.apache.pdfbox.rendering.PDFRenderer suggestKCMS\n",
      "INFO:   or call System.setProperty(\"sun.java2d.cmm\", \"sun.java2d.cmm.kcms.KcmsServiceProvider\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 205 rows from PDF tables\n",
      "Saved 22 chunks_100 -> data/index_merged\\chunks_100.json\n",
      "Saved 6 chunks_400 -> data/index_merged\\chunks_400.json\n",
      "Saved 28 merged chunks -> data/index_merged\\chunks_merged.json\n",
      "Embedding 28 docs with sentence-transformers/all-MiniLM-L6-v2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built & saved -> data/index_merged\\faiss_merged.index\n",
      "BM25 index built & saved -> data/index_merged\\bm25_merged.pkl\n",
      "Saved metadata -> data/index_merged\\meta_merged.pkl\n",
      "\n",
      "✅ Done. Created 100 + 400 chunks separately and merged them for unified FAISS & BM25 indexes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import tabula\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "PDF_PATH    = \"data/MakeMyTrip_Financial_Statements.pdf\"\n",
    "OUT_DIR     = \"data/index_merged\"\n",
    "\n",
    "# Paths for saved chunks & indices\n",
    "CHUNKS_100_PATH = os.path.join(OUT_DIR, \"chunks_100.json\")\n",
    "CHUNKS_400_PATH = os.path.join(OUT_DIR, \"chunks_400.json\")\n",
    "CHUNKS_MERGED_PATH = os.path.join(OUT_DIR, \"chunks_merged.json\")\n",
    "\n",
    "FAISS_PATH    = os.path.join(OUT_DIR, \"faiss_merged.index\")\n",
    "BM25_PATH     = os.path.join(OUT_DIR, \"bm25_merged.pkl\")\n",
    "META_PATH     = os.path.join(OUT_DIR, \"meta_merged.pkl\")\n",
    "\n",
    "# ---------------- Utils ----------------\n",
    "_tok_pat = re.compile(r\"[a-z0-9]+\", re.I)\n",
    "def simple_tokenize(text: str):\n",
    "    return _tok_pat.findall((text or \"\").lower())\n",
    "\n",
    "def create_chunks(texts: List[str], max_tokens: int) -> List[str]:\n",
    "    \"\"\"Simple word-based tokenizer to split texts into chunks.\"\"\"\n",
    "    chunks, current_chunk, current_tokens = [], [], 0\n",
    "    for text in texts:\n",
    "        tokens = re.findall(r\"\\w+\", text)\n",
    "        if current_tokens + len(tokens) > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk, current_tokens = [], 0\n",
    "        current_chunk.append(text)\n",
    "        current_tokens += len(tokens)\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path: str, pages=\"all\") -> List[Dict]:\n",
    "    \"\"\"Extract tables from financial PDF into structured row-year-value dicts.\"\"\"\n",
    "    tables = tabula.read_pdf(\n",
    "        pdf_path,\n",
    "        pages=pages,\n",
    "        multiple_tables=True,\n",
    "        pandas_options={'dtype': str}\n",
    "    )\n",
    "\n",
    "    table_rows = []\n",
    "    row_id = 0\n",
    "    \n",
    "    for df in tables:\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        df = df.replace(r'\\n', ' ', regex=True).fillna(\"\")\n",
    "\n",
    "        headers = list(df.iloc[0])\n",
    "        if any(re.match(r\"20\\d{2}\", str(c)) for c in headers):\n",
    "            df.columns = [c.strip() for c in headers]\n",
    "            df = df.drop(0).reset_index(drop=True)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            metric = str(row.iloc[0]).strip()\n",
    "            if not metric or metric.lower() in [\"note\", \"\"]:\n",
    "                continue\n",
    "\n",
    "            values = {}\n",
    "            for col, val in row.items():\n",
    "                if re.match(r\"20\\d{2}\", str(col)):\n",
    "                    clean_val = str(val).replace(\",\", \"\").strip()\n",
    "                    if clean_val and clean_val not in [\"-\", \"—\", \"nan\"]:\n",
    "                        values[str(col)] = clean_val\n",
    "\n",
    "            if not values:\n",
    "                continue\n",
    "\n",
    "            table_rows.append({\n",
    "                \"id\": f\"table-{row_id}\",\n",
    "                \"metric\": metric,\n",
    "                \"years\": list(values.keys()),\n",
    "                \"values\": values,\n",
    "                \"content\": f\"{metric} values: {json.dumps(values)}\",\n",
    "                \"source\": \"table\"\n",
    "            })\n",
    "            row_id += 1\n",
    "\n",
    "    print(f\"Extracted {len(table_rows)} rows from PDF tables\")\n",
    "    return table_rows\n",
    "\n",
    "def build_dense_faiss(texts: List[str], out_path: str):\n",
    "    print(f\"Embedding {len(texts)} docs with {EMBED_MODEL} ...\")\n",
    "    model = SentenceTransformer(EMBED_MODEL)\n",
    "    emb = model.encode(texts, convert_to_numpy=True, batch_size=64, show_progress_bar=True)\n",
    "    faiss.normalize_L2(emb)\n",
    "    dim = emb.shape[1]\n",
    "\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(emb)\n",
    "    faiss.write_index(index, out_path)\n",
    "    print(f\"FAISS index built & saved -> {out_path}\")\n",
    "\n",
    "def build_bm25(texts: List[str], out_path: str):\n",
    "    tokenized = [simple_tokenize(t) for t in texts]\n",
    "    bm25 = BM25Okapi(tokenized)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump({\"bm25\": bm25, \"tokenized_corpus\": tokenized}, f)\n",
    "    print(f\"BM25 index built & saved -> {out_path}\")\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # 1) Extract table rows\n",
    "    docs = extract_tables_from_pdf(PDF_PATH, pages=\"all\")\n",
    "    all_texts = [d[\"content\"] for d in docs]\n",
    "\n",
    "    # 2) Create chunks of size 100 and 400\n",
    "    chunks_100 = create_chunks(all_texts, 100)\n",
    "    chunks_400 = create_chunks(all_texts, 400)\n",
    "\n",
    "    # 3) Save them separately\n",
    "    with open(CHUNKS_100_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks_100, f, indent=2, ensure_ascii=False)\n",
    "    with open(CHUNKS_400_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks_400, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved {len(chunks_100)} chunks_100 -> {CHUNKS_100_PATH}\")\n",
    "    print(f\"Saved {len(chunks_400)} chunks_400 -> {CHUNKS_400_PATH}\")\n",
    "\n",
    "    # 4) Merge with metadata\n",
    "    merged = []\n",
    "    for i, ch in enumerate(chunks_100):\n",
    "        merged.append({\"id\": f\"100-{i}\", \"chunk_size\": 100, \"content\": ch})\n",
    "    for i, ch in enumerate(chunks_400):\n",
    "        merged.append({\"id\": f\"400-{i}\", \"chunk_size\": 400, \"content\": ch})\n",
    "\n",
    "    # 5) Save merged chunks\n",
    "    with open(CHUNKS_MERGED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved {len(merged)} merged chunks -> {CHUNKS_MERGED_PATH}\")\n",
    "\n",
    "    # 6) Build FAISS & BM25 on merged chunks\n",
    "    texts = [m[\"content\"] for m in merged]\n",
    "    build_dense_faiss(texts, FAISS_PATH)\n",
    "    build_bm25(texts, BM25_PATH)\n",
    "\n",
    "    # 7) Save metadata\n",
    "    with open(META_PATH, \"wb\") as f:\n",
    "        pickle.dump(merged, f)\n",
    "    print(f\"Saved metadata -> {META_PATH}\")\n",
    "\n",
    "    print(\"\\n✅ Done. Created 100 + 400 chunks separately and merged them for unified FAISS & BM25 indexes!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75476c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ punkt is already downloaded\n",
      "\n",
      "NLTK setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Download and verify NLTK data\n",
    "import os\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "\n",
    "def ensure_nltk_data():\n",
    "    # Set NLTK data path to user's home directory\n",
    "    nltk.data.path.append(os.path.expanduser(\"~/nltk_data\"))\n",
    "    \n",
    "    # Required NLTK data\n",
    "    required_data = ['punkt']\n",
    "    \n",
    "    # Download and verify each required dataset\n",
    "    for item in required_data:\n",
    "        try:\n",
    "            find(f'tokenizers/{item}')\n",
    "            print(f\"✓ {item} is already downloaded\")\n",
    "        except LookupError:\n",
    "            print(f\"Downloading {item}...\")\n",
    "            nltk.download(item)\n",
    "            print(f\"✓ {item} has been downloaded\")\n",
    "\n",
    "# Run the verification\n",
    "ensure_nltk_data()\n",
    "print(\"\\nNLTK setup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
