{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4304c100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kundankumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS, BM25, metadata, and models ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes and models loaded!\n",
      "[Guardrail] Accepted (semantic match 0.54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kundankumar/Documents/CAI/.venv/lib/python3.13/site-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the revenue from air ticketing for  year 2024?\n",
      "\n",
      "Final Answer:\n",
      " 201246\n",
      "\n",
      "Top supporting docs:\n",
      "[100-3] (chunk=100, score=1.601) -> Total current liabilities values: {\"2023\": \"453658\", \"2024\": \"297574\"} Total liabilities values: {\"2023\": \"483769\", \"202...\n",
      "[100-12] (chunk=100, score=-1.283) -> Hotels and packages values: {\"2023\": \"337686\", \"2024\": \"435542\", \"2025\": \"520411\"} Bus ticketing values: {\"2023\": \"74873...\n",
      "[100-11] (chunk=100, score=-1.500) -> Deferred tax liabilities, net values: {\"2024\": \"4754\", \"2025\": \"2526\"} Other non-current liabilities values: {\"2024\": \"1...\n",
      "[400-3] (chunk=400, score=-4.645) -> Total revenue values: {\"2023\": \"593036\", \"2024\": \"782524\", \"2025\": \"978336\"} Other income values: {\"2023\": \"2798\", \"2024...\n",
      "[100-15] (chunk=100, score=-5.389) -> Profit (loss) for the year values: {\"2023\": \"(11168)\", \"2024\": \"216743\", \"2025\": \"95274\"} Owners of the Company values: ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM, pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "# ---------------- Config ----------------\n",
    "EMBED_MODEL   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CROSS_ENCODER = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "QA_MODEL      = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "\n",
    "OUT_DIR     = \"data/index_merged\"\n",
    "FAISS_PATH  = os.path.join(OUT_DIR, \"faiss_merged.index\")\n",
    "BM25_PATH   = os.path.join(OUT_DIR, \"bm25_merged.pkl\")\n",
    "META_PATH   = os.path.join(OUT_DIR, \"meta_merged.pkl\")\n",
    "\n",
    "# ---------------- Load Indexes & Models ----------------\n",
    "print(\"Loading FAISS, BM25, metadata, and models ...\")\n",
    "\n",
    "faiss_index = faiss.read_index(FAISS_PATH)\n",
    "\n",
    "with open(BM25_PATH, \"rb\") as f:\n",
    "    bm25_obj = pickle.load(f)\n",
    "bm25 = bm25_obj[\"bm25\"]\n",
    "\n",
    "with open(META_PATH, \"rb\") as f:\n",
    "    meta: List[Dict] = pickle.load(f)\n",
    "\n",
    "embed_model = SentenceTransformer(EMBED_MODEL)\n",
    "reranker = CrossEncoder(CROSS_ENCODER)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=QA_MODEL,\n",
    "    tokenizer=QA_MODEL\n",
    ")\n",
    "\n",
    "print(\"Indexes and models loaded!\")\n",
    "\n",
    "# ---------------- Guardrails ----------------\n",
    "BLOCKED_TERMS = [\"weather\", \"cricket\", \"movie\", \"song\", \"football\", \"holiday\",\n",
    "                 \"travel\", \"recipe\", \"music\", \"game\", \"sports\", \"politics\", \"election\"]\n",
    "\n",
    "FINANCE_DOMAINS = [\n",
    "    \"financial reporting\", \"balance sheet\", \"income statement\",\n",
    "    \"assets and liabilities\", \"equity\", \"revenue\", \"profit and loss\",\n",
    "    \"goodwill impairment\", \"cash flow\", \"dividends\", \"taxation\",\n",
    "    \"investment\", \"valuation\", \"capital structure\", \"ownership interests\",\n",
    "    \"subsidiaries\", \"shareholders equity\", \"expenses\", \"earnings\",\n",
    "    \"debt\", \"amortization\", \"depreciation\"\n",
    "]\n",
    "finance_embeds = embed_model.encode(FINANCE_DOMAINS, convert_to_tensor=True)\n",
    "\n",
    "def validate_query(query: str, threshold: float = 0.5) -> bool:\n",
    "    q_lower = query.lower()\n",
    "    if any(bad in q_lower for bad in BLOCKED_TERMS):\n",
    "        print(\"[Guardrail] Rejected by blocklist.\")\n",
    "        return False\n",
    "    q_emb = embed_model.encode(query, convert_to_tensor=True)\n",
    "    sim_scores = util.cos_sim(q_emb, finance_embeds)\n",
    "    max_score = float(sim_scores.max())\n",
    "    if max_score > threshold:\n",
    "        print(f\"[Guardrail] Accepted (semantic match {max_score:.2f})\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"[Guardrail] Rejected (low semantic score {max_score:.2f})\")\n",
    "        return False\n",
    "\n",
    "def validate_output(answer: str, context_docs: List[Dict]) -> str:\n",
    "    combined_context = \" \".join([doc[\"content\"].lower() for doc in context_docs])\n",
    "    if answer.lower() in combined_context:\n",
    "        return answer\n",
    "    return \"The information could not be verified in the financial statements.\"\n",
    "\n",
    "def preprocess_query(query: str, remove_stopwords: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess query: clean, lowercase, optional stopword removal.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    query = query.lower()\n",
    "    # Remove non-alphanumeric (keep numbers, words, spaces)\n",
    "    query = re.sub(r\"[^a-z0-9\\s]\", \" \", query)\n",
    "    # Tokenize\n",
    "    tokens = query.split()\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ---------------- Hybrid Candidate Retrieval ----------------\n",
    "def hybrid_candidates(query: str, candidate_k: int = 50, alpha: float = 0.5) -> List[int]:\n",
    "    q_emb = embed_model.encode(\n",
    "    [preprocess_query(query, remove_stopwords=False)], \n",
    "    convert_to_numpy=True, \n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "    faiss_scores, faiss_ids = faiss_index.search(q_emb, max(candidate_k, 50))\n",
    "    faiss_ids = faiss_ids[0]\n",
    "    faiss_scores = faiss_scores[0]\n",
    "\n",
    "    tokenized_query = preprocess_query(query, remove_stopwords=True).split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    topN = max(candidate_k, 50)\n",
    "    bm25_top = np.argsort(bm25_scores)[::-1][:topN]\n",
    "    faiss_top = faiss_ids[:topN]\n",
    "    union_ids = np.unique(np.concatenate([bm25_top, faiss_top]))\n",
    "\n",
    "    faiss_score_map = {int(i): float(s) for i, s in zip(faiss_ids, faiss_scores)}\n",
    "    f_arr = np.array([faiss_score_map.get(int(i), -1.0) for i in union_ids], dtype=float)\n",
    "    f_min = np.min(f_arr)\n",
    "    if np.any(f_arr < 0):\n",
    "        f_arr = np.where(f_arr < 0, f_min, f_arr)\n",
    "    b_arr = np.array([bm25_scores[int(i)] for i in union_ids], dtype=float)\n",
    "\n",
    "    def _norm(x):\n",
    "        rng = np.ptp(x)\n",
    "        return (x - np.min(x)) / (rng + 1e-9)\n",
    "\n",
    "    f_norm = _norm(f_arr)\n",
    "    b_norm = _norm(b_arr)\n",
    "    combined = alpha * f_norm + (1 - alpha) * b_norm\n",
    "    order = np.argsort(combined)[::-1]\n",
    "    ranked_ids = union_ids[order][:candidate_k]\n",
    "    return ranked_ids.tolist()\n",
    "\n",
    "# ---------------- Cross-Encoder Rerank ----------------\n",
    "def rerank_cross_encoder(query: str, cand_ids: List[int], top_k: int = 10) -> List[Dict]:\n",
    "    pairs = [(query, meta[i][\"content\"]) for i in cand_ids]\n",
    "    scores = reranker.predict(pairs)\n",
    "    order = np.argsort(scores)[::-1][:top_k]\n",
    "    results = []\n",
    "    for rank_idx in order:\n",
    "        i = cand_ids[rank_idx]\n",
    "        results.append({\n",
    "            \"id\": meta[i][\"id\"],\n",
    "            \"chunk_size\": meta[i][\"chunk_size\"],\n",
    "            \"content\": meta[i][\"content\"],\n",
    "            \"rerank_score\": float(scores[rank_idx]),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ---------------- QA Answer Extraction via OpenAI Mistral ----------------\n",
    "def answer_question(query: str, context_docs: List[Dict]) -> str:\n",
    "    processed_query = query\n",
    "    context = \"\\n\".join([doc[\"content\"] for doc in context_docs])\n",
    "    qa_input = {\n",
    "        \"question\": processed_query,\n",
    "        \"context\": context\n",
    "    }\n",
    "    result = qa_pipeline(qa_input)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "\n",
    "# ---------------- Concept + Year Extraction ----------------\n",
    "def extract_value_for_year_and_concept(year: str, concept: str, context_docs: List[Dict]) -> str:\n",
    "    target_year = str(year)\n",
    "    concept_lower = concept.lower()\n",
    "\n",
    "    for doc in context_docs:\n",
    "        text = doc.get(\"content\", \"\")\n",
    "        # Split text into lines and filter\n",
    "        lines = [line for line in text.split(\"\\n\") if line.strip() and any(c.isdigit() for c in line)]\n",
    "        \n",
    "        header_idx = None\n",
    "        year_to_col = {}\n",
    "\n",
    "        # Step 1: Identify header line with years\n",
    "        for idx, line in enumerate(lines):\n",
    "            years_in_line = re.findall(r\"20\\d{2}\", line)\n",
    "            if years_in_line:\n",
    "                for col_idx, y in enumerate(years_in_line):\n",
    "                    year_to_col[y] = col_idx\n",
    "                header_idx = idx\n",
    "                break\n",
    "\n",
    "        if target_year not in year_to_col or header_idx is None:\n",
    "            continue\n",
    "\n",
    "        # Step 2: Find the line with the concept below header\n",
    "        for line in lines[header_idx+1:]:\n",
    "            if concept_lower in line.lower():\n",
    "                cols = re.split(r\"\\s{2,}|\\t\", line)\n",
    "                col_idx = year_to_col[target_year]\n",
    "                if col_idx < len(cols):\n",
    "                    value = cols[col_idx].replace(\",\", \"\")\n",
    "                    return value\n",
    "    return None\n",
    "\n",
    "# ---------------- End-to-End RAG Pipeline ----------------\n",
    "def rag_pipeline(query: str, top_k: int = 5, candidate_k: int = 50, alpha: float = 0.6):\n",
    "    if not validate_query(query):\n",
    "        return \"Query rejected: Please ask finance-related questions.\", []\n",
    "\n",
    "    cand_ids = hybrid_candidates(query, candidate_k=candidate_k, alpha=alpha)\n",
    "    reranked = rerank_cross_encoder(query, cand_ids, top_k=top_k)\n",
    "\n",
    "    year_match = re.search(r\"(20\\d{2})\", query)\n",
    "    year = year_match.group(0) if year_match else None\n",
    "    concept = re.sub(r\"for the year 20\\d{2}\", \"\", query, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    year_specific_answer = None\n",
    "    if year and concept:\n",
    "        year_specific_answer = extract_value_for_year_and_concept(year, concept, reranked)\n",
    "\n",
    "    if year_specific_answer:\n",
    "        answer = year_specific_answer\n",
    "    else:\n",
    "        answer = answer_question(query, reranked)\n",
    "\n",
    "    final_answer = validate_output(answer, reranked)\n",
    "    return final_answer, reranked\n",
    "\n",
    "# ---------------- Example ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"What is the revenue from air ticketing for  year 2024?\"\n",
    "    final_answer, top_docs = rag_pipeline(q, top_k=5, candidate_k=60, alpha=0.6)\n",
    "\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    print(\"\\nFinal Answer:\\n\", final_answer)\n",
    "    print(\"\\nTop supporting docs:\")\n",
    "    for r in top_docs:\n",
    "        print(f\"[{r['id']}] (chunk={r['chunk_size']}, score={r['rerank_score']:.3f}) -> {r['content'][:120]}...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
